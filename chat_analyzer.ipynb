{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import os    \n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "from string import *\n",
    "from additional_stopwords import ADDITIONAL_STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_DIR = os.getcwd() + \"/chats/\"\n",
    "USER_PATTERN = r'- ([^:]+): (.+)'\n",
    "LINK_PATTERN = r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,4}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)'\n",
    "LAUGH_PATTERN = r'\\b(?:\\w*(?:k\\w?k)+\\w*?)\\b'\n",
    "\n",
    "EMOJI_PATTERN = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('portuguese') + list(punctuation) + list(ADDITIONAL_STOPWORDS))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_by_chat(chat_names_list):\n",
    "    data_chats = {}\n",
    "    for chat_name in chat_names_list:\n",
    "        if chat_name.startswith(\".\"): continue\n",
    "        data_chats[chat_name] = organize_phrases_by_users(f'{CHAT_DIR}{chat_name}')\n",
    "    return data_chats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_phrases_by_users(file_path):      \n",
    "    chat_regex = re.compile(USER_PATTERN) \n",
    "    users_dict = {}\n",
    "    with open(file_path, \"r\") as file:\n",
    "        chat_lines = file.read().splitlines()\n",
    "        last_user = \"\"\n",
    "        for line in chat_lines:\n",
    "            match = chat_regex.search(line)\n",
    "            if match is None:\n",
    "                users_dict[last_user].append(line)\n",
    "                continue\n",
    "\n",
    "            user, phrase = match.groups()\n",
    "            if users_dict.get(user) is None:\n",
    "                users_dict[user] = []\n",
    "                continue\n",
    "            \n",
    "            users_dict[user].append(phrase)\n",
    "\n",
    "            last_user = user\n",
    "    return users_dict\n",
    "             \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(list_of_text):\n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "chat_names_list = os.listdir(CHAT_DIR)\n",
    "data_chats = get_data_by_chat(chat_names_list)\n",
    "\n",
    "for chat, users in data_chats.items():\n",
    "    data_chats[chat] = { key: [combine_text(value)] for (key, value) in users.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 150)\n",
    "data_chats_frame = pd.concat({ key: pd.DataFrame(value).T for key, value in data_chats.items() }, axis=0)\n",
    "\n",
    "data_chats_frame.columns = ['chats']\n",
    "data_chats_frame.chats.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import emoji\n",
    "\n",
    "def remove_emoji(text):\n",
    "    return emoji.get_emoji_regexp().sub(u'', text)\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\<.*?\\>', '', text)\n",
    "    text = re.sub(LINK_PATTERN, '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(LAUGH_PATTERN, '', text)\n",
    "    #text = remove_emoji(text)\n",
    "    text = re.sub('\\s\\s+', ' ', text)\n",
    "    text = re.sub(\"[‘’“”…']\", '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "lambda_round1 = lambda text: clean_text_round1(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the updated text\n",
    "data_clean = pd.DataFrame(data_chats_frame.chats.apply(lambda_round1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#download if not downloaded\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "only_words_data_frame = pd.DataFrame(data_clean.chats.apply(word_tokenize))\n",
    "\n",
    "#only_words_data_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def remove_stopwords(array_of_words):\n",
    "    return [word for word in array_of_words if word not in STOP_WORDS]\n",
    "\n",
    "final_data_df = pd.DataFrame(only_words_data_frame.chats.apply(remove_stopwords))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "get_most_common = lambda x: dict(FreqDist(x).most_common(200))\n",
    "frequency_df = pd.DataFrame(final_data_df.chats.apply(get_most_common))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['vai',\n 'aí',\n 'vou',\n 'tô',\n 'pq',\n 'tá',\n 'tbm',\n 'sim',\n 'boa',\n 'sei',\n 'acho',\n 'né',\n 'ainda',\n 'lá']"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "##Get most common words between users and exclude them\n",
    "\n",
    "words = []\n",
    "for chat, user in final_data_df.chats.items():\n",
    "    top_words_user_tuple = Counter(user).most_common(30) \n",
    "    top_words_user_list = [word for (word, count) in top_words_user_tuple]\n",
    "    for t in top_words_user_list:\n",
    "        words.append(t)\n",
    "\n",
    "add_stop_words = [word for word, count in Counter(words).most_common() if count > 3]\n",
    "add_stop_words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "##Setup wordclouds\n",
    "\n",
    "#print(add_stop_words)\n",
    "wc = WordCloud(stopwords=STOP_WORDS.union(add_stop_words), background_color=\"white\", colormap=\"Dark2\",\n",
    "               max_font_size=150, random_state=42)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create wordclouds\n",
    "for index, (chat, user) in enumerate(final_data_df.chats.items()):    \n",
    "    wc.generate(\" \".join(user))\n",
    "    plt.subplot(3,2, index+1)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(chat[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitanaconda3virtualenvd8c8899c62dc4fb1a3c4bc4ff8eb2858",
   "display_name": "Python 3.7.6 64-bit ('anaconda3': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}